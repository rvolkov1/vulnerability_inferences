import os
import csv
import numpy as np
import tsm_local
import json

global_names = {}

network_num = 2

tweet_id = "1116612649972719617"
label = "F"
tweet_id2 = "1117354119650598912"
label2 = "R"

train_output_dir = "n2_train"
test_output_dir = "n2_test"
timestamp_folder = "timestamp_folder"

network_tsm_input_file = f"network_{network_num}_TSM_FORMAT.txt"
network_tsm_output_file = f"network_{network_num}_TSM_OUTPUT.txt"

neighborhood_size = 10

train_base_output_filepath = f"../GAT-GCN-SpreaderPrediction/{train_output_dir}/{timestamp_folder}/{train_output_dir}_"

test_base_output_filepath = f"../GAT-GCN-SpreaderPrediction/{test_output_dir}/{timestamp_folder}/{test_output_dir}_"

os.makedirs(train_base_output_filepath, exist_ok=True)
os.makedirs(test_base_output_filepath, exist_ok=True)

# generate dictonary of all names in network, assign index to each name, and record all of the output connections of each node

input_files = [f"clean_network_{tweet_id}_{label}.txt", f"clean_network_{tweet_id2}_{label2}.txt"]

for file in input_files:
  with open(file, 'r') as input_file:
    r_file = csv.reader(input_file)

    for row in r_file:
      for i, name in enumerate(row):
        if (name not in global_names):
          global_names[name] = {}
          global_names[name]["nbrs"] = []
        elif(i == 0):
          global_names[name]["nbrs"].append(row[1])

print(len(global_names.keys()))

local_trust_keys = []

with open("local_trust_N2.json", 'r') as local_trust_file:
  data = json.load(local_trust_file)
  local_trust_keys = list(data.keys())


false_spreaders = {}
refutation_spreaders = {}

# get all labeled users

with open(f"retweets_{tweet_id2}.txt") as f:
  lines = f.readlines()

  for line in lines:
    line = line.split(",")
    if (line[2] not in global_names):
      continue
    
    if (len(global_names[line[2]]["nbrs"]) == 0):
      continue

    refutation_spreaders[line[2]] = True # refutation spreader

with open(f"retweets_{tweet_id}.txt") as f:
  lines = f.readlines()

  for line in lines:
    line = line.split(",")

    if (line[2] not in global_names):
      continue
    false_spreaders[line[2]] = False # false spreader

print("false spreader count", len(false_spreaders.keys()))
print("refutation spreader count", len(refutation_spreaders.keys()))

# balance network, remove nodes with fewest neighbors from larger network

def remove_bottom_users(users, target_count):
  # get bottom users by out_degree
  l = sorted(users.keys(), key = lambda name: len(global_names[name]["nbrs"]), reverse=True)[target_count:]

  for name in l:
    del users[name]

if (len(refutation_spreaders.keys()) > len(false_spreaders.keys())):
  remove_bottom_users(refutation_spreaders, len(false_spreaders.keys()))
else:
  remove_bottom_users(false_spreaders, len(refutation_spreaders.keys()))

assert len(refutation_spreaders.keys()) == len(false_spreaders.keys())
print("len each spreader dict", len(refutation_spreaders.keys()))
  
# now that the network is balanced, we can save the network

labeled_users = {}
network_names = {}

# add all labeled users and 10 best neighbors to network
retweet_files = [f"retweets_{tweet_id}.txt", f"retweets_{tweet_id2}.txt"]

for file in retweet_files:
  with open(file, 'r') as r_file:
    r_lines = r_file.readlines()

    for line in r_lines:
      user = line.split(",")[2]

      if user not in false_spreaders and user not in refutation_spreaders or (user in false_spreaders and user in refutation_spreaders):
        continue

      labeled_users[user] = global_names[user]
      
      # check this is the future
      if file == f"retweets_1116612649972719617.txt":
        labeled_users[user]["refutation"] = False
      else:
        labeled_users[user]["refutation"] = True

index = 0

print("labeled users", len(labeled_users.keys()))

clean_networks = [f"clean_network_{tweet_id}_{label}.txt", f"clean_network_{tweet_id2}_{label2}.txt"]
# save network in TSM_readable format
with open(network_tsm_input_file, 'w') as output_file:
  w_file = csv.writer(output_file, delimiter = "\t")

  for file in clean_networks:
    with open(file, 'r') as f:
      r_file = csv.reader(f)

      for line in r_file:
        if len(line) < 2:
          print("bad line:", line)
          continue

        name1 = line[0]
        name2 = line[1]
      
        if name1 not in labeled_users and name2 not in labeled_users:
          continue
        
        if name1 not in network_names.keys():
          network_names[name1] = global_names[name1]
          network_names[name1]["index"] = index
          network_names[name1]["degree"] = len(global_names[name1]["nbrs"])
          index += 1

        if name2 not in network_names.keys():
          network_names[name2] = global_names[name2]
          network_names[name2]["index"] = index
          network_names[name2]["degree"] = len(global_names[name2]["nbrs"])
          index += 1
        
        for name in (name1, name2):
          for n in global_names[name]["nbrs"]:
            if (n not in network_names):
              network_names[n] = global_names[n]
              network_names[n]["index"] = index
              network_names[n]["degree"] = len(global_names[n]["nbrs"])
              index += 1
      
        row = [network_names[name1]["index"], network_names[name2]["index"]]
        w_file.writerow(row)

#for user in network_names:
#  network_names[user]["nbrs"] = [name for name in network_names[user]["nbrs"] if name in network_names]
#  print(len(network_names[user]["nbrs"]))

# re-create network_names accounting for the nodes actually in the TSM_OUTPUT

print("len network names", len(network_names))
print("len network names set", len(set(network_names.keys())))
print("len labeled names", len(labeled_users))

# re-run TSM if needed
if False:
  tsm_local.run(network_tsm_input_file, network_tsm_output_file)

# save training out
trust_arr = []

# generate matrix of local and global trust from files made previously
with open("local_trust_N2.json", 'r') as local_trust_file:
  data = json.load(local_trust_file)
  data_keys = list(data.keys())

  with open(network_tsm_output_file, "r") as global_trust_file:
    global_trust_lines = global_trust_file.readlines()

    len1 = len(global_trust_lines)
    len2 = len(data)

    for i in range(min(len1, len2)):
      local_trust = data[data_keys[i]]
      global_trust = global_trust_lines[i]

      curr_arr = []

      curr_arr.append(float(local_trust["trusting_others"]))
      curr_arr.append(float(local_trust["trusted_by_others"]))

      global_trust = global_trust.split(",")

      curr_arr.append(float(global_trust[1]))
      curr_arr.append(float(global_trust[2]))
      
      trust_arr.append(curr_arr)

# save trust arr to train and test directories
trust_arr = np.array(trust_arr)
print("\n\ntrust.npy")
print(trust_arr)
print(trust_arr.dtype)
np.save(train_base_output_filepath + "trust.npy", trust_arr)
np.save(test_base_output_filepath + "trust.npy", trust_arr)

# create nbr and adj matrices for training input
nbr = []
adj = []
labels = []

num_neighbors = 10

for name in labeled_users.keys():
  #print(name)

  l = sorted(network_names[name]["nbrs"], key=lambda x: len(network_names[x]["nbrs"]), reverse=True) # sort users in neighborhood by their respective in_degreess in descending order
  l = l[:num_neighbors] + [0]*(num_neighbors-len(l)) # truncate / pad list to 10 elements

  label = 1 if labeled_users[name]["refutation"] else 0 # 1 if refutation spreader 0 if false info spreader
  labels.append(label)

  curr_adj = []

  for user1 in l:
    user_adj = []

    for user2 in l:
      if (user1 == 0 or user2 ==0):
        user_adj.append(0)
      elif (user1 == user2):
        user_adj.append(0)
      else:
        if user2 in network_names[user1]["nbrs"]:
          user_adj.append(1)
        else:
          user_adj.append(0)
      
    curr_adj.append(np.array(user_adj, dtype=np.uint32))
  
  adj.append(np.array(curr_adj))
              
  l = list(map(lambda x: 0 if x == 0 else network_names[x]["index"], l)) # express each username with unique id

  nbr.append(np.array(l))
    

# save training arrays
print("\n\n train nbr.npy")
nbr = np.array(nbr)
print(nbr)
print(np.shape(nbr))
np.save(train_base_output_filepath + "nbr.npy", nbr)

print("\n\ntrain adj.npy")
print(adj)
print(np.shape(adj))
adj = np.array(adj)
np.save(train_base_output_filepath + "adj.npy", adj)

print("\n\ntrain label.npy")
print(labels)
print(np.shape(labels))
labels = np.array(labels, dtype=np.int64)
np.save(train_base_output_filepath + "label.npy", labels)

# get credibilty features for all global users
with open ("N2_full_data.txt") as f:
  for line in f:
    line = line.strip()
    line = line.split(",")

    if (len(line) != 8):
      print("bad data line")
      continue
      
    if (line[0] not in network_names):
      continue
    
    cred_vals = line[1:]
    cred_vals = [float(x) for x in cred_vals]

    network_names[line[0]]["cred"] = cred_vals
  
cred_mat = []

# generate credibilyt matrix
for name in labeled_users.keys():
  subarr = []

  l = sorted(network_names[name]["nbrs"], key=lambda x: len(network_names[x]["nbrs"]), reverse=True) # sort users in neighborhood by their respective in_degreess in descending order
  l = l[:num_neighbors] + [0]*(num_neighbors-len(l)) # truncate / pad list to 50 elements

  for neighbor in l:
    if (neighbor == 0):
      subarr.append(np.zeros(7, dtype=np.int32))
    else:
      subarr.append(np.array(network_names[neighbor]["cred"]))
  
  cred_mat.append(np.array(subarr))

cred_mat = np.array(cred_mat)
print(cred_mat)
print(cred_mat.shape)
print(cred_mat.dtype)
np.save(train_base_output_filepath + "cred.npy", cred_mat)

##########################
#
# generate test_files
#
##########################

nbr = []
adj = []

num_neighbors = 10

from tqdm import tqdm

for name, _ in tqdm(sorted(network_names.items(), key=lambda x: x[1]['index'])):
  #print(name, contents)

  l = sorted(network_names[name]["nbrs"], key=lambda x: network_names[name]["degree"], reverse=True) # sort users in neighborhood by their respective in_degreess in descending order
  l = l[:num_neighbors] + [0]*(num_neighbors-len(l)) # truncate / pad list to 10 elements

  for i, name in enumerate(l):
    if name not in network_names:
      l[i] = 0

  curr_adj = []

  for user1 in l:
    user_adj = []

    for user2 in l:
      if (user1 == 0 or user2 ==0):
        user_adj.append(0)
      elif (user1 == user2):
        user_adj.append(0)
      else:
        if user2 in network_names[user1]["nbrs"]:
          user_adj.append(1)
        else:
          user_adj.append(0)
      
    curr_adj.append(np.array(user_adj, dtype=np.uint32))
  
  adj.append(np.array(curr_adj))
              
  l = list(map(lambda x: 0 if x == 0 else network_names[x]["index"], l)) # express each username with unique id

  nbr.append(np.array(l))
    
#print("len retweet names", len(set(retweet_names.keys())))

nbr = np.array(nbr)
np.save(test_base_output_filepath + "nbr.npy", nbr)

adj = np.array(adj)
np.save(test_base_output_filepath + "adj.npy", adj)


labels = np.zeros(len(set(network_names.keys())), dtype=np.int64)
np.save(test_base_output_filepath + "label.npy", labels)

print("done")

cred_mat = []

# get credibilty features for all global users
with open ("N2_full_data.txt") as f:
  for line in tqdm(f):
    line = line.strip()
    line = line.split(",")

    if (len(line) != 8):
      print("bad data line")
      continue
      
    if (line[0] not in network_names):
      continue
    
    cred_vals = line[1:]
    cred_vals = [float(x) for x in cred_vals]

    network_names[line[0]]["cred"] = cred_vals
  
# generate credibilyt matrix
for name in tqdm(network_names):
  subarr = []

  l = sorted(network_names[name]["nbrs"], key=lambda x: len(global_names[x]["nbrs"]), reverse=True) # sort users in neighborhood by their respective in_degreess in descending order
  l = l[:num_neighbors] + [0]*(num_neighbors-len(l)) # truncate / pad list to 50 elements

  for neighbor in l:
    if (neighbor == 0):
      subarr.append(np.zeros(7, dtype=np.int32))
    else:
      subarr.append(np.array(network_names[neighbor]["cred"]))
  
  cred_mat.append(np.array(subarr))

cred_mat = np.array(cred_mat)
print(cred_mat)
print(cred_mat.shape)
print(cred_mat.dtype)
np.save(test_base_output_filepath + "cred.npy", cred_mat)