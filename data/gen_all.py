import csv
import numpy as np

global_names = {}

tweet_id = "1116612649972719617"
label = "F"

#tweet_id = "1117354119650598912"
#label = "R"

# generate dictonary of all names in network, assign index to each name, and record all of the output connections of each node
with open(f"clean_network_{tweet_id}_{label}.txt", 'r') as input_file, open(f"clean_network_{tweet_id}_{label}_TSM_FORMAT.txt", 'w') as output_file:
  r_file = csv.reader(input_file)
  w_file = csv.writer(output_file, delimiter = "\t")

  next_key = 0

  for row in r_file:
    for i, name in enumerate(row):
      if (name not in global_names):
        global_names[name] = {}
        global_names[name]["index"] = next_key
        global_names[name]["nbrs"] = []
        next_key += 1
      elif(i == 0):
        global_names[name]["nbrs"].append(row[1])
      
      row[i] = global_names[name]["index"]

    w_file.writerow(row)
  print("len global names", len(global_names))

nbr = []
adj = []

num_neighbors = 10

for name, contents in sorted(global_names.items(), key=lambda x: x[1]['index']):
  #print(name, contents)

  l = sorted(global_names[name]["nbrs"], key=lambda x: len(global_names[x]["nbrs"]), reverse=True) # sort users in neighborhood by their respective in_degreess in descending order
  l = l[:num_neighbors] + [0]*(num_neighbors-len(l)) # truncate / pad list to 10 elements

  curr_adj = []

  for user1 in l:
    user_adj = []

    for user2 in l:
      if (user1 == 0 or user2 ==0):
        user_adj.append(0)
      elif (user1 == user2):
        user_adj.append(0)
      else:
        if user2 in global_names[user1]["nbrs"]:
          user_adj.append(1)
        else:
          user_adj.append(0)
      
    curr_adj.append(np.array(user_adj, dtype=np.uint32))
  
  adj.append(np.array(curr_adj))
              
  l = list(map(lambda x: 0 if x == 0 else global_names[x]["index"], l)) # express each username with unique id

  nbr.append(np.array(l))
    
#print("len retweet names", len(set(retweet_names.keys())))

nbr = np.array(nbr)
np.save("nbr.npy", nbr)

adj = np.array(adj)
np.save("adj.npy", adj)

labels = np.zeros(nbr.shape[0])
np.save("label.npy", labels)

print("\n\nnbr.npy")
arr = np.load("nbr.npy")
print(arr)
print(np.shape(arr))

print("\n\nadj.npy")
arr = np.load("adj.npy")
print(arr)
print(np.shape(arr))

print("\n\nlabel.npy")
arr = np.load("label.npy")
print(arr)
print(np.shape(arr))

cred_mat = []

# get credibilty features for all global users
with open ("N2_full_data.txt") as f:
  for line in f:
    line = line.strip()
    line = line.split(",")

    if (len(line) != 8):
      print("bad data line")
      continue
      
    if (line[0] not in global_names):
      continue
    
    cred_vals = line[1:]
    cred_vals = [float(x) for x in cred_vals]

    global_names[line[0]]["cred"] = cred_vals
  
# generate credibilyt matrix
for name in global_names:
  subarr = []

  l = sorted(global_names[name]["nbrs"], key=lambda x: len(global_names[x]["nbrs"]), reverse=True) # sort users in neighborhood by their respective in_degreess in descending order
  l = l[:num_neighbors] + [0]*(num_neighbors-len(l)) # truncate / pad list to 50 elements

  for neighbor in l:
    if (neighbor == 0):
      subarr.append(np.zeros(7), dtype=numpy.int32)
    else:
      subarr.append(np.array(global_names[neighbor]["cred"]))
  
  cred_mat.append(np.array(subarr))

cred_mat = np.array(cred_mat)
print(cred_mat)
print(cred_mat.shape)
print(cred_mat.dtype)
np.save("cred.npy", cred_mat)